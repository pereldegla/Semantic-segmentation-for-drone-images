# Semantic-segmentation-for-drone-images
My goal was to get a better understanding in semantic segmentation and how to build a U-net model from scratch. There is room for improvement so feel free to reach me at pereldegla@gmail.com 

It is basically an encoder-decoder architecture with skip connections. 

<img src=https://user-images.githubusercontent.com/46407601/155888192-1cfde578-a8d1-4ce9-9557-3d7fcd8c7e46.png alt="alt text" width=450 height=332>

Aerial semantic segmentation using urban scenes for increasing the safety of autonomous drone flight and landing procedures. 
The imagery depicts more than 20 houses from nadir (bird's eye) view acquired at an altitude of 5 to 30 meters above ground. A high resolution camera was used to acquire images at a size of 6000x4000px (24Mpx). The training set contains 400 publicly available images and the test set is made up of 200 private images.

My custom model showed an 80% accuracy. More insight to be added soon. 
![image](https://user-images.githubusercontent.com/46407601/155888358-4dacb9bf-1226-4af8-beec-c3ef122645a3.png)

Here are some predictions:
![image](https://user-images.githubusercontent.com/46407601/155888372-6d40a876-cd1a-4112-83d9-7c48f9c308b2.png)
![image](https://user-images.githubusercontent.com/46407601/155888391-1a53c26f-827f-45b2-b4ba-804d7dba0e35.png)

Link to the dataset https://www.kaggle.com/bulentsiyah/semantic-drone-dataset
